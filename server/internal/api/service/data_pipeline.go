//go:generate moq -out data_pipeline_moq.go . IDataLogPipeline
package service

import (
	"context"
	"database/sql"
	"encoding/xml"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/asaskevich/govalidator"
	"github.com/georgysavva/scany/v2/sqlscan"
	"github.com/rimdian/rimdian/internal/api/entity"
	"github.com/rimdian/rimdian/internal/api/repository"
	"github.com/rimdian/rimdian/internal/common/dto"
	common "github.com/rimdian/rimdian/internal/common/dto"
	"github.com/rimdian/rimdian/internal/common/httpClient"
	"github.com/rimdian/rimdian/internal/common/taskorchestrator"
	"github.com/rotisserie/eris"
	"github.com/sirupsen/logrus"
	"github.com/tidwall/gjson"
	"go.opencensus.io/trace"
)

type IDataLogPipeline interface {
	// Pipeline interface
	Cfg() *entity.Config
	Log() *logrus.Logger
	Net() httpClient.HTTPClient
	Repo() repository.Repository
	GetWorkspace() *entity.Workspace
	GetQueueResult() *common.DataLogInQueueResult
	Execute(ctx context.Context)
	ProcessNextStep(ctx context.Context)
	InsertChildDataLog(ctx context.Context, data entity.ChildDataLog) error
	EnsureUsersLock(ctx context.Context) error
	ReleaseUsersLock() error
	GetUserIDs() []string
	AddDataLogGenerated(dataLog *entity.DataLog)
	SetError(key string, err string, retryable bool)
	HasError() bool

	// DataLogPipeline interface
	GetDataLog() *entity.DataLog
	GetDataLogsGenerated() []*entity.DataLog
	LoadFxRates(ctx context.Context) error
	Replay(ctx context.Context)
	InitDataLog(ctx context.Context)
	CreateDataLogFromQueue(ctx context.Context)
	ExtractAndValidateItem(ctx context.Context)

	StepPending(ctx context.Context)
	StepPersistDatalog(ctx context.Context)
	StepUpsertItem(ctx context.Context)
	StepExecuteSpecialAction(ctx context.Context)
	StepAttribution(ctx context.Context)
	StepSegmentation(ctx context.Context)
	StepWorkflows(ctx context.Context)
	StepHookFinalize(ctx context.Context)

	ExtractUserFromDataLogItem()
	ExtractUserAliasFromDataLogItem()
	ExtractDeviceFromDataLogItem(userID string)
	ExtractSessionFromDataLogItem(isMandatory bool)
	ExtractPageviewFromDataLogItem()
	ExtractOrderFromDataLogItem()
	ExtractCartFromDataLogItem()
	ExtractCustomEventFromDataLogItem()
	ExtractPostviewFromDataLogItem()
	ExtractSubscriptionListUserFromDataLogItem(ctx context.Context)
	ExtractAppItemFromDataLogItem()
	ExtractExtraColumnsFromItem(kind string)
	ParseUserAgent(userAgent string) (result *entity.UserAgentResult, err error)

	UpsertUser(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertUserAlias(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertPageview(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertOrder(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertCart(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertCustomEvent(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertDevice(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertSession(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertPostview(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertSubscriptionListUser(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertMessage(ctx context.Context, isChild bool, tx *sql.Tx) (err error)
	UpsertAppItem(ctx context.Context, isChild bool, tx *sql.Tx) (err error)

	ReattributeUsersOrders(ctx context.Context)
	ComputeSegmentsForGivenUsers(ctx context.Context)
	AttributeOrder(ctx context.Context, order *entity.Order, orderSessions []*entity.Session, orderPostviews []*entity.Postview, previousOrders []*entity.Order, devices []*entity.Device, tx *sql.Tx)
	DataLogEnqueue(ctx context.Context, replayID *string, origin int, originID string, workspaceID string, jsonItems []string, isSync bool)
}

type DataLogPipeline struct {
	Config           *entity.Config
	Logger           *logrus.Logger
	NetClient        httpClient.HTTPClient
	Repository       repository.Repository
	TaskOrchestrator taskorchestrator.Client
	Workspace        *entity.Workspace
	Apps             []*entity.App
	// data received from the queue
	DataLogInQueue *dto.DataLogInQueue
	// data_log generated & persisted from the dDataLogInQueue
	DataLog     *entity.DataLog
	QueueResult *dto.DataLogInQueueResult
	// user ids impacted by the data_log that need to be locked
	// in order to serialize the data_log processing at the user leve
	// and guarantee idempotency of the data_log
	UsersLock *entity.UsersLock
	// list of data_logs children generated by the current data_log
	// ie: a pageview can upsert a session, user, device, user_alias, enter/exit segment...
	// all of these events might eventually trigger workflows at the end of the pipeline
	DataLogsGenerated []*entity.DataLog
}

func (pipe *DataLogPipeline) Cfg() *entity.Config {
	return pipe.Config
}

func (pipe *DataLogPipeline) Log() *logrus.Logger {
	return pipe.Logger
}

func (pipe *DataLogPipeline) Net() httpClient.HTTPClient {
	return pipe.NetClient
}

func (pipe *DataLogPipeline) Repo() repository.Repository {
	return pipe.Repository
}

func (pipe *DataLogPipeline) GetWorkspace() *entity.Workspace {
	return pipe.Workspace
}

func (pipe *DataLogPipeline) GetQueueResult() *dto.DataLogInQueueResult {
	return pipe.QueueResult
}

// create a new data_log from the data_log in the queue
// extract the JSON item, validate data & populate the DataLog object
func (pipe *DataLogPipeline) CreateDataLogFromQueue(ctx context.Context) {

	_, span := trace.StartSpan(ctx, "DataLogPipeline.CreateDataLogFromQueue")
	defer span.End()

	now := time.Now()

	// init dataLog
	pipe.DataLog = &entity.DataLog{
		ID:         pipe.DataLogInQueue.ID,
		Origin:     pipe.DataLogInQueue.Origin,
		OriginID:   pipe.DataLogInQueue.OriginID,
		Context:    pipe.DataLogInQueue.Context,
		Item:       pipe.DataLogInQueue.Item,
		Checkpoint: entity.DataLogCheckpointPending,
		// will be overwritten by the item created_at / updated_at
		// but we need to set it in case the item has an error and should be persisted anyway
		EventAt:      now,
		EventAtTrunc: now.Truncate(time.Hour),
		// inits
		Errors:        entity.MapOfStrings{},
		Hooks:         entity.DataHooksState{},
		UpdatedFields: entity.UpdatedFields{},
	}

	// extract item kind
	result := gjson.Get(pipe.DataLog.Item, "kind")
	if !result.Exists() {
		pipe.DataLog.Kind = "unknown"
		// report error but continue to persist the data_log in DB
		pipe.SetError("kind", fmt.Sprintf("doDataLog: item has no kind, %v", pipe.DataLog.Item), false)
		return
	}

	pipe.DataLog.Kind = strings.TrimSpace(result.String())

	// check kind
	// cart_item and order_item can't be "parent" data_logs
	if !govalidator.IsIn(pipe.DataLog.Kind,
		"user",
		"user_alias",
		"device",
		"session",
		"pageview",
		"order",
		"cart",
		"postview",
		"custom_event",
		"subscription_list_user",
		"message") {
		// check if kind starts with "app_"
		if strings.HasPrefix(pipe.DataLog.Kind, "app_") || strings.HasPrefix(pipe.DataLog.Kind, "appx_") {
			return
		}
		// report error but continue to persist the data_log in DB
		pipe.SetError("kind", fmt.Sprintf("doDataLog: item kind not supported: %v", pipe.DataLog.Kind), false)
		return
	}
}

func (pipe *DataLogPipeline) GetDataLog() *entity.DataLog {
	return pipe.DataLog
}

func (pipe *DataLogPipeline) GetDataLogsGenerated() []*entity.DataLog {
	return pipe.DataLogsGenerated
}

func (pipe *DataLogPipeline) LoadFxRates(ctx context.Context) (err error) {

	spanCtx, span := trace.StartSpan(ctx, "DataLogPipeline.LoadFxRates")
	defer span.End()

	// fetch fx rate if not provided
	dateMax := time.Now().Add(-24 * time.Hour)
	if pipe.Workspace.FxRates == nil || pipe.Workspace.FxRates.UpdatedAt.Before(dateMax) {

		// dev data
		data := []byte(`<?xml version="1.0" encoding="UTF-8"?>
		<gesmes:Envelope xmlns:gesmes="http://www.gesmes.org/xml/2002-08-01" xmlns="http://www.ecb.int/vocabulary/2002-08-01/eurofxref">
			<gesmes:subject>Reference rates</gesmes:subject>
			<gesmes:Sender>
				<gesmes:name>European Central Bank</gesmes:name>
			</gesmes:Sender>
			<Cube>
				<Cube time="2023-03-24">
					<Cube currency="USD" rate="1.0745"/>
					<Cube currency="JPY" rate="139.85"/>
					<Cube currency="BGN" rate="1.9558"/>
					<Cube currency="CZK" rate="23.682"/>
					<Cube currency="DKK" rate="7.4519"/>
					<Cube currency="GBP" rate="0.87940"/>
					<Cube currency="HUF" rate="386.80"/>
					<Cube currency="PLN" rate="4.6865"/>
					<Cube currency="RON" rate="4.9305"/>
					<Cube currency="SEK" rate="11.2080"/>
					<Cube currency="CHF" rate="0.9874"/>
					<Cube currency="ISK" rate="150.10"/>
					<Cube currency="NOK" rate="11.3065"/>
					<Cube currency="TRY" rate="20.4882"/>
					<Cube currency="AUD" rate="1.6189"/>
					<Cube currency="BRL" rate="5.7298"/>
					<Cube currency="CAD" rate="1.4816"/>
					<Cube currency="CNY" rate="7.3826"/>
					<Cube currency="HKD" rate="8.4344"/>
					<Cube currency="IDR" rate="16316.05"/>
					<Cube currency="ILS" rate="3.8755"/>
					<Cube currency="INR" rate="88.5650"/>
					<Cube currency="KRW" rate="1401.12"/>
					<Cube currency="MXN" rate="20.0854"/>
					<Cube currency="MYR" rate="4.7579"/>
					<Cube currency="NZD" rate="1.7329"/>
					<Cube currency="PHP" rate="58.421"/>
					<Cube currency="SGD" rate="1.4330"/>
					<Cube currency="THB" rate="36.748"/>
					<Cube currency="ZAR" rate="19.5755"/>
				</Cube>
			</Cube>
		</gesmes:Envelope>`)

		if pipe.Config.ENV != entity.ENV_DEV {

			// fetch currency rates from ECB
			req, _ := http.NewRequest("GET", "http://www.ecb.europa.eu/stats/eurofxref/eurofxref-daily.xml", nil)

			resp, err := pipe.NetClient.Do(req)

			if err != nil {
				pipe.Logger.Printf("error get new rates: %v", err)
				return eris.Wrap(err, "LoadFxRates")
			}

			defer resp.Body.Close()

			// fmt.Printf("response Status: %v", resp.Status)
			// fmt.Printf("response Headers: %v", resp.Header)
			data, _ = io.ReadAll(resp.Body)
			// fmt.Printf("response Body: %+v\n", string(b))
		}

		var newRatesXML entity.FxCurrencyXML

		if err := xml.Unmarshal(data, &newRatesXML); err != nil {
			pipe.Logger.Printf("error unmarshal new rates: %v", err)
			return eris.Wrap(err, "LoadFxRates")
		}

		// fmt.Printf("newRatesXML: %+v\n", newRatesXML)
		if len(newRatesXML.Cube.Items) == 0 {
			return eris.New("error fetching new rates")
		}

		newRates := &entity.FxRates{
			Base:       "EUR",
			DateString: newRatesXML.Cube.Time,
			Rates:      map[string]float64{},
			UpdatedAt:  time.Now(),
		}

		for _, item := range newRatesXML.Cube.Items {
			newRates.Rates[item.Currency] = item.Rate
		}

		pipe.Workspace.FxRates = newRates

		// persist fx rates
		if err := pipe.Repository.UpdateWorkspace(spanCtx, pipe.Workspace, nil); err != nil {
			return eris.Wrap(err, "LoadFxRates")
		}
	}

	return nil
}

func (pipe *DataLogPipeline) Replay(ctx context.Context) {

	spanCtx, span := trace.StartSpan(ctx, "DataLogPipeline.Replay")
	defer span.End()

	var err error

	dataLogID := pipe.DataLogInQueue.ID

	// Replay() can be called on a duplicate data_log
	if !pipe.DataLogInQueue.IsReplay {
		dataLogID = dto.ComputeDataLogID(pipe.Config.SECRET_KEY, pipe.DataLogInQueue.Origin, pipe.DataLogInQueue.Item)
	}

	// fetch dataLog from DB
	if pipe.DataLog, err = pipe.Repository.GetDataLog(spanCtx, pipe.Workspace.ID, dataLogID); err != nil {
		// check if not found
		if sqlscan.NotFound(err) {
			pipe.SetError("server", fmt.Sprintf("doReplayDataLog: dataLog not found: %v", dataLogID), false)
			return
		}

		// unexpected DB error, retry
		pipe.SetError("server", fmt.Sprintf("doReplayDataLog: %v", err), true)
		return
	}

	// mark dataLog as done if it should not be replayed
	if !pipe.DataLog.IsReplayable() {
		pipe.DataLog.Checkpoint = entity.DataLogCheckpointDone
		if err = pipe.Repository.UpdateDataLog(spanCtx, pipe.Workspace.ID, pipe.DataLog); err != nil {
			pipe.SetError("server", fmt.Sprintf("doReplayDataLog: %v", err), true)
		}
		return
	}

	pipe.DataLogInQueue.IsReplay = true

	// ignore if dataLog is already done without errors
	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointDone && pipe.DataLog.HasError == entity.DataLogHasErrorNone {
		return
	}

	// reset dataLog errors
	pipe.DataLog.HasError = entity.DataLogHasErrorNone
	pipe.DataLog.Errors = entity.MapOfStrings{}

	// rehydrate dataLog for not yet upserted items
	if pipe.DataLog.Checkpoint < entity.DataLogCheckpointItemUpserted {
		pipe.ExtractAndValidateItem(spanCtx)
	}

	if pipe.DataLog.Checkpoint >= entity.DataLogCheckpointItemUpserted && pipe.DataLog.UserID != entity.None {
		// fetch user
		user, err := pipe.Repository.FindUserByID(spanCtx, pipe.Workspace, pipe.DataLog.UserID, nil)
		if err != nil && !sqlscan.NotFound(err) {
			pipe.SetError("server", fmt.Sprintf("doReplayDataLog: %v", err), true)
			return
		}
		pipe.DataLog.UpsertedUser = user
	}

	pipe.AddDataLogGenerated(pipe.DataLog)

	// fetch data_log children from DB
	children, err := pipe.Repository.GetDataLogChildren(spanCtx, pipe.Workspace.ID, pipe.DataLog.ID)
	if err != nil {
		pipe.SetError("server", fmt.Sprintf("doReplayDataLog: %v", err), true)
		return
	}

	// add children to the list of generated data_logs
	for _, child := range children {
		pipe.AddDataLogGenerated(child)
	}
}

func (pipe *DataLogPipeline) Execute(ctx context.Context) {

	spanCtx, span := trace.StartSpan(ctx, "DataLogPipeline.Execute")
	defer span.End()

	span.AddAttributes(
		trace.StringAttribute("data_log_in_queue.id", pipe.DataLogInQueue.ID),
		trace.Int64Attribute("data_log_in_queue.origin", int64(pipe.DataLogInQueue.Origin)),
		trace.StringAttribute("data_log_in_queue.origin_id", pipe.DataLogInQueue.OriginID),
		trace.BoolAttribute("data_log_in_queue.is_replay", pipe.DataLogInQueue.IsReplay),
	)

	defer func() {

		// update dataLog in DB if exist
		if pipe.DataLog != nil {
			bgCtx := context.Background()
			if pipe.DataLog.IsPersisted() {
				if err := pipe.Repository.UpdateDataLog(bgCtx, pipe.Workspace.ID, pipe.DataLog); err != nil {
					pipe.SetError("server", fmt.Sprintf("doDataLog: %v", err), true)
				}
			} else if pipe.HasError() {
				// persist invalid datalog in DB to keep track of the error
				pipe.Repository.RunInTransactionForWorkspace(bgCtx, pipe.Workspace.ID, func(ctx context.Context, tx *sql.Tx) (txCode int, txErr error) {
					pipe.Repository.InsertDataLog(ctx, pipe.Workspace.ID, pipe.DataLog, tx)
					return 200, nil
				})
			}
		}

		// release user lock
		if err := pipe.ReleaseUsersLock(); err != nil {
			pipe.Logger.Println(err)
		}
	}()

	pipe.ProcessNextStep(spanCtx)
}

func (pipe *DataLogPipeline) ProcessNextStep(ctx context.Context) {

	spanCtx, span := trace.StartSpan(ctx, "DataLogPipeline.ProcessNextStep")
	defer span.End()

	// stop processing if there is an error in a previous step
	if pipe.HasError() {
		pipe.Logger.Printf("error in previous step, aborting: %+v\n", pipe.QueueResult)
		return
	}

	// new data_log or replay
	if pipe.DataLog == nil {
		pipe.InitDataLog(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointPending {
		pipe.StepPending(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointHookOnValidationExecuted {
		if pipe.DataLogInQueue.IsReplay {
			pipe.DataLog.Checkpoint = entity.DataLogCheckpointPersisted
		} else {
			pipe.StepPersistDatalog(spanCtx)
		}
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointPersisted {
		pipe.StepUpsertItem(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointItemUpserted {
		pipe.StepExecuteSpecialAction(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointSpecialActionExecuted {
		pipe.StepAttribution(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointConversionsAttributed {
		pipe.StepSegmentation(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointSegmentsRecomputed {
		pipe.StepWorkflows(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointWorkflowsTriggered {
		pipe.StepHookFinalize(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	if pipe.DataLog.Checkpoint == entity.DataLogCheckpointHooksFinalizeExecuted {
		pipe.StepEnd(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}
}

func (pipe *DataLogPipeline) DataLogEnqueue(ctx context.Context, replayID *string, origin int, originID string, workspaceID string, jsonItems []string, isSync bool) {
	err := DataLogEnqueue(ctx, pipe.Config, pipe.NetClient, replayID, origin, originID, workspaceID, jsonItems, isSync)
	if err != nil {
		pipe.SetError("server", err.Error(), false)
	}
}

func (pipe *DataLogPipeline) ReattributeUsersOrders(ctx context.Context) {
	ReattributeUsersOrders(ctx, pipe)
}

func (pipe *DataLogPipeline) ComputeSegmentsForGivenUsers(ctx context.Context) {
	ComputeSegmentsForGivenUsers(ctx, pipe)
}

func (pipe *DataLogPipeline) AttributeOrder(ctx context.Context, order *entity.Order, orderSessions []*entity.Session, orderPostviews []*entity.Postview, previousOrders []*entity.Order, devices []*entity.Device, tx *sql.Tx) {
	AttributeOrder(ctx, pipe, order, orderSessions, orderPostviews, previousOrders, devices, tx)
}

func (pipe *DataLogPipeline) InitDataLog(ctx context.Context) {
	// span
	spanCtx, span := trace.StartSpan(ctx, "DataLogPipeline.InitDataLog")
	defer span.End()

	// refresh/fetch fx rates
	if err := pipe.LoadFxRates(spanCtx); err != nil {
		pipe.SetError("server", fmt.Sprintf("doDataLog: %v", err), true)
		return
	}

	// load apps

	apps, err := pipe.Repository.ListApps(ctx, pipe.Workspace.ID)

	if err != nil {
		pipe.SetError("server", fmt.Sprintf("doDataLog: %v", err), true)
		return
	}

	pipe.Apps = apps

	// check if the dataLog is a replay of a previous data import
	if pipe.DataLogInQueue.IsReplay {
		pipe.Replay(spanCtx)
		return
	}

	// compute a hash ID from its "item" and compares it with existing ID
	computedID := dto.ComputeDataLogID(pipe.Config.SECRET_KEY, pipe.DataLogInQueue.Origin, pipe.DataLogInQueue.Item)

	if computedID != pipe.DataLogInQueue.ID {
		pipe.Logger.Printf("dropping data: ID integrity check failed: %+v\n", pipe.DataLogInQueue)
		pipe.SetError("id", fmt.Sprintf("id integrity check failed: %+v\n != %v", pipe.DataLogInQueue.Item, computedID), false)
		return
	}

	pipe.CreateDataLogFromQueue(spanCtx)
}

// an error in the item will end the processing of the data_log
func (pipeline *DataLogPipeline) SetError(key string, err string, shouldRetry bool) {
	pipeline.QueueResult.SetError(err, shouldRetry)
	if pipeline.DataLog != nil {
		pipeline.DataLog.HasError = entity.DataLogHasErrorNotRetryable
		pipeline.DataLog.Errors[key] = err
		if shouldRetry {
			pipeline.DataLog.HasError = entity.DataLogHasErrorRetryable
		}
	}
}

func (pipeline *DataLogPipeline) HasError() bool {
	return pipeline.QueueResult.HasError
}

func (pipeline *DataLogPipeline) GetUserIDs() []string {
	return pipeline.UsersLock.UserIDs
}

func (pipeline *DataLogPipeline) ReleaseUsersLock() error {
	return pipeline.Repository.ReleaseUsersLock(pipeline.Workspace.ID, pipeline.UsersLock)
}

func (pipe *DataLogPipeline) EnsureUsersLock(ctx context.Context) error {
	return pipe.Repository.EnsureUsersLock(ctx, pipe.Workspace.ID, pipe.UsersLock, true)
}

// generate a child DataLog, persist it in DB and add it to the list of generated data_logs
func (pipe *DataLogPipeline) InsertChildDataLog(ctx context.Context, data entity.ChildDataLog) (err error) {

	childDataLog := entity.NewInternalDataLogChild(pipe.DataLog, data)

	// determine if this child data_log should be already considered as "done" or not
	childDataLog.Checkpoint = entity.DataLogCheckpointDone

	// TODO: check if has workflows and set the checkpoint accordingly

	for _, hook := range pipe.Workspace.DataHooks {
		// only on_success hooks here
		if (hook.On == entity.DataHookKindOnSuccess && hook.Enabled && childDataLog.HasError == 0) &&
			hook.MatchesDataLogKind(childDataLog.Kind, childDataLog.Action) {

			// init hooks state if nil
			if childDataLog.Hooks == nil {
				childDataLog.Hooks = entity.DataHooksState{}
			}

			// set default hook state
			childDataLog.Hooks[hook.ID] = &entity.DataHookState{
				Done: false,
			}

			// set checkpoint if has no workflows before
			if childDataLog.Checkpoint != entity.DataLogCheckpointDone {
				childDataLog.Checkpoint = entity.DataLogCheckpointWorkflowsTriggered
			}
		}
	}

	if err = pipe.Repository.InsertDataLog(ctx, childDataLog.Context.WorkspaceID, childDataLog, data.Tx); err != nil {
		return err
	}

	pipe.AddDataLogGenerated(childDataLog)
	return nil
}

func (pipeline *DataLogPipeline) AddDataLogGenerated(dataLog *entity.DataLog) {
	pipeline.DataLogsGenerated = append(pipeline.DataLogsGenerated, dataLog)
}

type DataPipelineProps struct {
	Config           *entity.Config
	Logger           *logrus.Logger
	NetClient        httpClient.HTTPClient
	Repository       repository.Repository
	TaskOrchestrator taskorchestrator.Client
	Workspace        *entity.Workspace
	DataLogInQueue   *dto.DataLogInQueue
}

func NewDataPipeline(props *DataPipelineProps) IDataLogPipeline {

	return &DataLogPipeline{
		Config:            props.Config,
		Logger:            props.Logger,
		NetClient:         props.NetClient,
		Repository:        props.Repository,
		TaskOrchestrator:  props.TaskOrchestrator,
		Workspace:         props.Workspace,
		DataLogInQueue:    props.DataLogInQueue,
		QueueResult:       &dto.DataLogInQueueResult{},
		UsersLock:         entity.NewUsersLock(),
		DataLogsGenerated: []*entity.DataLog{},
	}
}
