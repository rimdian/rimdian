//go:generate moq -out data_pipeline_moq.go . DataPipeline
package service

import (
	"context"
	"database/sql"
	"fmt"
	"runtime/debug"
	"strings"
	"time"

	"github.com/georgysavva/scany/v2/sqlscan"
	"github.com/google/uuid"
	"github.com/rimdian/rimdian/internal/api/dto"
	"github.com/rimdian/rimdian/internal/api/entity"
	"github.com/rimdian/rimdian/internal/api/repository"
	common "github.com/rimdian/rimdian/internal/common/dto"
	"github.com/rimdian/rimdian/internal/common/httpClient"
	"github.com/rimdian/rimdian/internal/common/taskorchestrator"
	"github.com/rotisserie/eris"
	"github.com/sirupsen/logrus"
	"go.opencensus.io/trace"
)

type ITaskExecPipeline interface {
	// Pipeline interface
	Cfg() *entity.Config
	Log() *logrus.Logger
	Net() httpClient.HTTPClient
	Repo() repository.Repository
	GetWorkspace() *entity.Workspace
	GetQueueResult() *common.ResponseForTaskQueue
	Execute(ctx context.Context)
	ProcessNextStep(ctx context.Context)
	InsertChildDataLog(ctx context.Context, data entity.ChildDataLog) error
	EnsureUsersLock(ctx context.Context) error
	ReleaseUsersLock() error
	GetUserIDs() []string
	AddDataLogGenerated(dataLog *entity.DataLog)
	GetDataLogsGenerated() []*entity.DataLog
	SetError(key string, err string, retryable bool)
	HasError() bool

	// TaskExecPipeline interface
	Init(ctx context.Context)
	TaskExecAddWorker(ctx context.Context, workerID int, initialWorkerState entity.TaskWorkerState)

	// ComputeSegmentsForGivenUsers()
	ReattributeUsersOrders(ctx context.Context)
	AttributeOrder(ctx context.Context, order *entity.Order, orderSessions []*entity.Session, orderPostviews []*entity.Postview, previousOrders []*entity.Order, devices []*entity.Device, tx *sql.Tx)
	DataLogEnqueue(ctx context.Context, replayID *string, origin int, originID string, workspaceID string, jsonItems []string, isSync bool)
}

type TaskExecPipeline struct {
	Config           *entity.Config
	Logger           *logrus.Logger
	NetClient        httpClient.HTTPClient
	Repository       repository.Repository
	Workspace        *entity.Workspace
	TaskOrchestrator taskorchestrator.Client
	// data received from the queue
	TaskExecPayload *dto.TaskExecRequestPayload
	// data_log generated & persisted from the DataLogInQueue
	Task           *entity.Task
	TaskExec       *entity.TaskExec
	TaskExecResult *entity.TaskExecResult
	QueueResult    *common.ResponseForTaskQueue
	// user ids impacted by the task that need to be locked
	// in order to serialize the data_log processing at the user level
	// and guarantee idempotency of the data_log
	UsersLock *entity.UsersLock
	// list of data_logs children generated by the current data_log
	// ie: a pageview can upsert a session, user, device, user_alias, enter/exit segment...
	// all of these events might eventually trigger workflows at the end of the pipeline
	DataLogsGenerated []*entity.DataLog
}

func (pipe *TaskExecPipeline) Cfg() *entity.Config {
	return pipe.Config
}

func (pipe *TaskExecPipeline) Log() *logrus.Logger {
	return pipe.Logger
}

func (pipe *TaskExecPipeline) Net() httpClient.HTTPClient {
	return pipe.NetClient
}

func (pipe *TaskExecPipeline) Repo() repository.Repository {
	return pipe.Repository
}

func (pipe *TaskExecPipeline) GetWorkspace() *entity.Workspace {
	return pipe.Workspace
}

func (pipe *TaskExecPipeline) GetQueueResult() *common.ResponseForTaskQueue {
	return pipe.QueueResult
}

func (pipe *TaskExecPipeline) Execute(ctx context.Context) {

	spanCtx, span := trace.StartSpan(ctx, "Execute")
	defer span.End()

	span.AddAttributes(
		trace.StringAttribute("task_exec_payload.id", pipe.TaskExecPayload.TaskExecID),
		trace.Int64Attribute("task_exec_payload.worker_id", int64(pipe.TaskExecPayload.WorkerID)),
		trace.StringAttribute("task_exec_payload.job_id", pipe.TaskExecPayload.JobID),
	)

	// catch non-200 codes & panics to save it in DB
	// thats why var "code" is updated before returning, to be visible in defer
	defer func() {

		// release user lock
		if err := pipe.ReleaseUsersLock(); err != nil {
			pipe.Logger.Println(err)
		}

		if pipe.HasError() && pipe.TaskExec != nil && pipe.TaskExec.IsPersisted() {

			status := entity.TaskExecStatusAborted
			pipe.TaskExec.Message = entity.StringPtr(pipe.QueueResult.Error)

			if pipe.QueueResult.QueueShouldRetry {
				status = entity.TaskExecStatusRetryingError
				// on error 500 sleep 15 secs to avoid spamming the API again
				// if request does timeout its fine too...
				if pipe.Config.ENV != entity.ENV_DEV {
					time.Sleep(15 * time.Second)
				}
			}

			// use background context to avoid timeout while persisting results
			if err := pipe.Repository.SetTaskExecError(context.Background(), pipe.Workspace.ID, pipe.TaskExec.ID, pipe.TaskExecPayload.WorkerID, status, pipe.QueueResult.Error); err != nil {
				pipe.Logger.Println(err)
				// task queue will retry at this point...
			}
		}

		if r := recover(); r != nil {
			fmt.Println("task recover stacktrace: \n" + string(debug.Stack()))

			// might have a not-yet-persisted error
			message := pipe.QueueResult.Error

			if !pipe.HasError() {
				// in case of context timeout, return the error
				pipe.SetError("recover", fmt.Sprintf("recover: %v", message), true)
				message = fmt.Sprintf("will retry err: %v", r)
			}

			if err := pipe.Repository.SetTaskExecError(context.Background(), pipe.Workspace.ID, pipe.TaskExec.ID, pipe.TaskExecPayload.WorkerID, entity.TaskExecStatusRetryingError, message); err != nil {
				pipe.Logger.Println(err)
				pipe.SetError("server", err.Error(), true) // db error, should retry
				return
			}
		}
	}()

	pipe.ProcessNextStep(spanCtx)
}

func (pipe *TaskExecPipeline) ProcessNextStep(ctx context.Context) {

	spanCtx, span := trace.StartSpan(ctx, "ProcessNextStep")
	defer span.End()

	// stop processing if there is an internal error in a previous step
	if pipe.HasError() {
		return
	}

	// fetch task exec
	if pipe.TaskExec == nil {
		pipe.Init(spanCtx)
		pipe.ProcessNextStep(spanCtx)
		return
	}

	// the task_exec has returned a result, save it
	if pipe.TaskExecResult != nil {

		// queue result depends on the task result
		if pipe.TaskExecResult.IsError {
			pipe.QueueResult = &common.ResponseForTaskQueue{
				HasError:         pipe.TaskExecResult.IsError,
				Error:            *pipe.TaskExecResult.Message,
				QueueShouldRetry: true,
			}
			if pipe.TaskExecResult.IsDone {
				pipe.QueueResult.QueueShouldRetry = false
			}
		}

		pipe.TaskExecResult.WorkerID = pipe.TaskExecPayload.WorkerID

		bgCtx := context.Background()

		// enqueue items if any
		if pipe.TaskExecResult.ItemsToImport != nil && len(pipe.TaskExecResult.ItemsToImport) > 0 {
			pipe.DataLogEnqueue(bgCtx, nil, 4, pipe.TaskExec.ID, pipe.Workspace.ID, pipe.TaskExecResult.ItemsToImport, false)
			if pipe.HasError() {
				pipe.ProcessNextStep(spanCtx)
				return
			}
		}

		// use background context to avoid timeout while persisting results
		_, err := pipe.Repository.RunInTransactionForWorkspace(bgCtx, pipe.Workspace.ID, func(ctx context.Context, tx *sql.Tx) (int, error) {

			if subErr := pipe.Repository.UpdateTaskExecFromResult(ctx, pipe.TaskExecPayload, pipe.TaskExecResult, tx); subErr != nil {
				return 500, subErr
			}

			// mutate app state if provided
			if pipe.TaskExecResult.AppStateMutations != nil && len(pipe.TaskExecResult.AppStateMutations) > 0 && (strings.HasPrefix(pipe.TaskExec.TaskID, "app_") || strings.HasPrefix(pipe.TaskExec.TaskID, "appx_")) {

				// extract app id from task kind
				bits := strings.Split(pipe.TaskExec.TaskID, "_")
				if len(bits) > 2 {
					appID := bits[0] + "_" + bits[1]
					// get app
					app, err := pipe.Repository.GetApp(ctx, pipe.Workspace.ID, appID)

					if err != nil {
						if sqlscan.NotFound(err) {
							return 400, err
						}
						return 500, eris.Wrap(err, "ProcessNextStep")
					}

					// apply mutations
					app.ApplyMutations(pipe.TaskExecResult.AppStateMutations)
					app.UpdatedAt = time.Now()

					if err := pipe.Repository.UpdateApp(ctx, app, tx); err != nil {
						if sqlscan.NotFound(err) {
							return 400, err
						}
						return 500, eris.Wrap(err, "ProcessNextStep")
					}
				}
			}

			// pipe.Logger.Printf("update task results %+v", result)

			// enqueue next task if has more to do
			if !pipe.TaskExecResult.IsDone {
				// compute a new job id
				id, err := uuid.NewRandom()
				if err != nil {
					return 500, err
				}

				newJobID := fmt.Sprintf("%v_%v", id.String(), pipe.TaskExec.TaskID) // random id first, for Google performance sharding

				if subErr := pipe.Repository.AddJobToTaskExec(ctx, pipe.TaskExecPayload.TaskExecID, newJobID, tx); subErr != nil {
					return 500, subErr
				}

				job := &taskorchestrator.TaskRequest{
					QueueLocation:     pipe.Config.TASK_QUEUE_LOCATION,
					QueueName:         entity.TaskExecsQueueName,
					PostEndpoint:      pipe.Config.API_ENDPOINT + entity.TaskExecEndpoint + "?workspace_id=" + pipe.Workspace.ID,
					TaskTimeoutInSecs: &entity.TaskTimeoutInSecs,
					Payload: dto.TaskExecRequestPayload{
						TaskExecID: pipe.TaskExec.ID,
						WorkerID:   pipe.TaskExecPayload.WorkerID,
						JobID:      newJobID,
					},
				}

				// add eventual delay for polling
				// (main threads = workers[0] do polling to check when all workers are done to end the task)
				if pipe.TaskExecResult.DelayNextRequestInSecs != nil {
					scheduledAt := time.Now().Add(time.Duration(*pipe.TaskExecResult.DelayNextRequestInSecs) * time.Second)
					job.ScheduleTime = &scheduledAt
					// pipe.Logger.Printf("now %v, schedule time %v", time.Now(), *job.ScheduleTime)
				}

				if err := pipe.TaskOrchestrator.PostRequest(ctx, job); err != nil {
					return 500, err
				}

				// pipe.Logger.Printf("enqueued next task %s", task.ID)
			}

			return 200, nil
		})

		if err != nil {
			pipe.SetError("server", err.Error(), true)
			pipe.ProcessNextStep(spanCtx)
			return
		}

		// saved results, stop processing
		return
	}

	if pipe.TaskExec.Status == entity.TaskExecStatusProcessing {

		switch pipe.TaskExec.TaskID {
		case entity.TaskKindGenerateDemo:
			pipe.TaskExecResult = TaskExecGenerateDemo(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindDataLogReprocessUntil:
			pipe.TaskExecResult = TaskExecDataLogReprocessUntil(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindReattributeConversions:
			pipe.TaskExecResult = TaskExecReattributeConversions(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindRecomputeSegment:
			pipe.TaskExecResult = TaskExecRecomputeSegment(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindImportUsersToSubscriptionList:
			pipe.TaskExecResult = TaskExecImportUsersToSubscriptionList(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindUpgradeApp:
			pipe.TaskExecResult = TaskExecUpgradeApp(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindLaunchBroadcastCampaign:
			pipe.TaskExecResult = TaskExecLaunchBroadcastCampaign(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindTestingNotDone:
			pipe.TaskExecResult = &entity.TaskExecResult{IsDone: false}
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindTestingDone:
			pipe.TaskExecResult = &entity.TaskExecResult{IsDone: true}
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindTestingTimeout:
			pipe.SetError("server", "timeout", true)
			pipe.ProcessNextStep(spanCtx)
			return
		case entity.TaskKindTestingPanic:
			panic("testing panic")

		default:
			// check if task.Kind starts with "app_" or "appx_"
			if !strings.HasPrefix(pipe.TaskExec.TaskID, "app_") && !strings.HasPrefix(pipe.TaskExec.TaskID, "appx_") {
				pipe.SetError("server", fmt.Sprintf("unknown task kind: %v", pipe.TaskExec.TaskID), false)
				pipe.ProcessNextStep(spanCtx)
				return
			}

			pipe.TaskExecResult = TaskExecCallAppWebhook(spanCtx, pipe)
			pipe.ProcessNextStep(spanCtx)
			return
		}
	}
}

func (pipe *TaskExecPipeline) Init(ctx context.Context) {

	spanCtx, span := trace.StartSpan(ctx, "Init")
	defer span.End()

	// fetch taskExec
	var err error
	pipe.TaskExec, err = pipe.Repository.GetTaskExec(spanCtx, pipe.Workspace.ID, pipe.TaskExecPayload.TaskExecID)

	if err != nil {
		if eris.Is(err, entity.ErrTaskExecNotFound) {
			pipe.SetError("server", err.Error(), false)
			return
		}

		pipe.SetError("server", err.Error(), true)
		return
	}

	// in dev we can use the "todo" task ID to fetch a not-yet-done task from the DB
	if pipe.TaskExecPayload.TaskExecID == entity.TaskExecIDDev {
		pipe.TaskExecPayload.TaskExecID = pipe.TaskExec.ID
	}

	// abort if task is already completed
	if pipe.TaskExec.Status == entity.TaskExecStatusDone {
		return
	}

	// abort task if too many retries
	if pipe.TaskExec.RetryCount > entity.TaskMaxRetries {
		// keep original error message and wrap it with count retry
		msg := ""
		if pipe.TaskExec.Message != nil {
			msg = *pipe.TaskExec.Message
		}
		pipe.SetError("retry", fmt.Sprintf("(%v, %v) %v", entity.ErrTaskRetryCountExceeded, pipe.TaskExec.RetryCount, msg), false)
		return
	}

	pipe.TaskExec.ResetStatus()
}

// add a parallel worker for a given task
// each worker has its own JSON state persisted in DB
func (pipe *TaskExecPipeline) TaskExecAddWorker(ctx context.Context, workerID int, initialWorkerState entity.TaskWorkerState) {

	spanCtx, span := trace.StartSpan(ctx, "TaskExecAddWorker")
	defer span.End()

	if workerID == 0 {
		// should not happen
		pipe.SetError("server", entity.ErrTaskWorkerIDNotAllowed.Error(), false)
		return
	}

	_, err := pipe.Repo().RunInTransactionForWorkspace(spanCtx, pipe.Workspace.ID, func(ctx context.Context, tx *sql.Tx) (int, error) {

		// compute a new job id
		id, err := uuid.NewRandom()
		if err != nil {
			return 500, err
		}

		newJobID := fmt.Sprintf("%v_%v", id.String(), pipe.TaskExec.TaskID) // random id first, for Google performance sharding

		// add worker state / jobid in task
		if err := pipe.Repo().AddTaskExecWorker(ctx, pipe.TaskExec.ID, newJobID, workerID, initialWorkerState, tx); err != nil {
			return 500, err
		}

		// enqueue job
		job := &taskorchestrator.TaskRequest{
			QueueLocation:     pipe.Config.TASK_QUEUE_LOCATION,
			QueueName:         entity.TaskExecsQueueName,
			PostEndpoint:      pipe.Config.API_ENDPOINT + entity.TaskExecEndpoint + "?workspace_id=" + pipe.Workspace.ID,
			TaskTimeoutInSecs: &entity.TaskTimeoutInSecs,
			Payload: dto.TaskExecRequestPayload{
				TaskExecID: pipe.TaskExec.ID,
				WorkerID:   workerID,
				JobID:      newJobID,
			},
		}

		if err := pipe.TaskOrchestrator.PostRequest(ctx, job); err != nil {
			return 500, err
		}

		return 200, nil
	})

	if err != nil {
		pipe.SetError("server", err.Error(), true)
	}
}

func (pipe *TaskExecPipeline) ReattributeUsersOrders(ctx context.Context) {
	ReattributeUsersOrders(ctx, pipe)
}

func (pipe *TaskExecPipeline) AttributeOrder(ctx context.Context, order *entity.Order, orderSessions []*entity.Session, orderPostviews []*entity.Postview, previousOrders []*entity.Order, devices []*entity.Device, tx *sql.Tx) {
	AttributeOrder(ctx, pipe, order, orderSessions, orderPostviews, previousOrders, devices, tx)
}

func (pipe *TaskExecPipeline) DataLogEnqueue(ctx context.Context, replayID *string, origin int, originID string, workspaceID string, jsonItems []string, isSync bool) {
	err := DataLogEnqueue(ctx, pipe.Config, pipe.NetClient, replayID, origin, originID, workspaceID, jsonItems, isSync)
	if err != nil {
		pipe.SetError("server", err.Error(), false)
	}
}

// an error in the item will end the processing of the data_log
func (pipe *TaskExecPipeline) SetError(key string, err string, shouldRetry bool) {
	pipe.Logger.Printf("TaskExecPipeline error %v: %v", key, err)
	pipe.QueueResult.SetError(err, shouldRetry)
	if pipe.TaskExec != nil {
		pipe.TaskExec.Message = entity.StringPtr(err)
		pipe.TaskExec.Status = entity.TaskExecStatusAborted
		if shouldRetry {
			pipe.TaskExec.Status = entity.TaskExecStatusRetryingError
		}
	}
}

func (pipe *TaskExecPipeline) HasError() bool {
	return pipe.QueueResult.HasError
}

func (pipe *TaskExecPipeline) GetUserIDs() []string {
	return pipe.UsersLock.UserIDs
}

func (pipe *TaskExecPipeline) ReleaseUsersLock() error {
	return pipe.Repository.ReleaseUsersLock(pipe.Workspace.ID, pipe.UsersLock)
}

func (pipe *TaskExecPipeline) EnsureUsersLock(ctx context.Context) error {
	return pipe.Repository.EnsureUsersLock(ctx, pipe.Workspace.ID, pipe.UsersLock, true)
}

// generate a child DataLog, persist it in DB and add it to the list of generated data_logs
// tasks can generate massive amounts of data_log children
// for that reason they will trigger the eventual workflows+hooks asynchronously at the end  of the pipeline
func (pipe *TaskExecPipeline) InsertChildDataLog(ctx context.Context, data entity.ChildDataLog) (err error) {

	childDataLog := entity.NewTaskDataLog(pipe.Workspace.ID, pipe.Task.ID, data)

	// determine if this child data_log should be already considered as "done" or not
	childDataLog.Checkpoint = entity.DataLogCheckpointDone

	// TODO: check if has workflows and set the checkpoint DataLogCheckpointShouldRespawn

	for _, hook := range pipe.Workspace.DataHooks {
		// only on_success hooks here
		if (hook.On == entity.DataHookKindOnSuccess && hook.Enabled && childDataLog.HasError == 0) &&
			hook.MatchesDataLogKind(childDataLog.Kind, childDataLog.Action) {

			// init hooks state if nil
			if childDataLog.Hooks == nil {
				childDataLog.Hooks = entity.DataHooksState{}
			}

			// set default hook state
			childDataLog.Hooks[hook.ID] = &entity.DataHookState{
				Done: false,
			}

			// set checkpoint to "should respawn" if hook is not done
			childDataLog.Checkpoint = entity.DataLogCheckpointShouldRespawn
		}
	}

	if err = pipe.Repository.InsertDataLog(ctx, childDataLog.Context.WorkspaceID, childDataLog, data.Tx); err != nil {
		return err
	}

	pipe.AddDataLogGenerated(childDataLog)
	return nil
}

func (pipeline *TaskExecPipeline) AddDataLogGenerated(dataLog *entity.DataLog) {
	pipeline.DataLogsGenerated = append(pipeline.DataLogsGenerated, dataLog)
}
func (pipeline *TaskExecPipeline) GetDataLogsGenerated() []*entity.DataLog {
	return pipeline.DataLogsGenerated
}

type TaskExecPipelineProps struct {
	Config           *entity.Config
	Logger           *logrus.Logger
	NetClient        httpClient.HTTPClient
	Repository       repository.Repository
	Workspace        *entity.Workspace
	TaskOrchestrator taskorchestrator.Client
	TaskExecPayload  *dto.TaskExecRequestPayload
}

func NewTaskExecPipeline(props *TaskExecPipelineProps) ITaskExecPipeline {
	return &TaskExecPipeline{
		Config:            props.Config,
		Logger:            props.Logger,
		NetClient:         props.NetClient,
		Repository:        props.Repository,
		Workspace:         props.Workspace,
		TaskOrchestrator:  props.TaskOrchestrator,
		TaskExecPayload:   props.TaskExecPayload,
		QueueResult:       &common.ResponseForTaskQueue{},
		UsersLock:         entity.NewUsersLock(),
		DataLogsGenerated: []*entity.DataLog{},
	}
}
